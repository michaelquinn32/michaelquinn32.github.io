spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>% str
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
map_call(rbind.data.frame) %>%
kable
spam_data %>% count(target) %>% kable
split_data(spam_data) %>% str
split_data <- function(.data, .test_rate = 0.1) {
n <- nrow(.data)
svec <- sample(c("train", "test"), n, replace = TRUE, prob = c(1 - .test_rate, .test_rate))
split(.data, svec)
}
split_data(spam_data) %>% str
split_data(spam_data) %>% str
?MASS::lda
library(caret)
library(caret)
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda', trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'lda',
trControl = TC, tuneGrid = NB_pars))
models
test <- invoke(models, data = spam_data)
test <- invoke_map(models, data = spam_data)
test <- mutate(spam, target = as.factor(target))
test <- mutate(spam_data, target = as.factor(target))
lda1 <- LDA(target ~ ., spam_data)
score_model(lda1)
lda1 <- LDA(target ~ ., test)
score_model(lda1)
lda1
library(adventureR)
lda1 <- LDA(target ~ ., test)
score_model(lda1)
str(lda1)
detach("package:adventureR", unload=TRUE)
library("adventureR", lib.loc="/usr/local/lib/R/3.2/site-library")
library(adventureR)
detach("package:adventureR", unload=TRUE)
# Implementing Spam experiment
# Prelims -----------------------------------------------------------------
library(tidyr)
library(dplyr)
library(purrr)
library(readr)
library(knitr)
library(ggplot2)
library(caret)
library(adventureR)
options(digits = 4)
# Links -------------------------------------------------------------------
spam_http <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spam_names <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"
# Get Data ----------------------------------------------------------------
col_names <- read_lines(spam_names, skip = 33) %>% gsub(":.*", "", .) %>% c("target")
col_types <- c(replicate(55, col_double()), replicate(3, col_integer()))
spam_data <- read_csv(spam_http, col_names = col_names, col_types = col_types)
# Fix target --------------------------------------------------------------
spam_data <- mutate(spam_data, target = as.factor(target))
# Basic EDA ---------------------------------------------------------------
summary_funs <- list(n = length,
nmiss = compose(sum, is.na),
mean = partial(mean, na.rm = TRUE),
sd = partial(sd, na.rm = TRUE),
min = partial(min, na.rm = TRUE),
p25 = partial(quantile, probs = .25, names = FALSE, na.rm = TRUE),
median = partial(median, na.rm = TRUE),
p75 = partial(quantile, probs = .75, names = FALSE, na.rm = TRUE),
max = partial(max, na.rm = TRUE))
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
invoke(rbind.data.frame, .) %>%
kable
spam_data %>% count(target) %>% kable
# Implementing the Experiment ---------------------------------------------
# Store models to be tested
TC <- trainControl(method = 'none')
NB_pars <- data.frame(fL = 0, usekernel = FALSE)
proc <- c("nzv")
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC, preProcess = proc),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
# Create list of random training and test dataframes
dfs <- rerun(10, split_data(spam_data))
# Fit and score models
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
results
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration")
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>% as.data.frame
test <- naive_bayes(target ~ . , dfs[[4]]$train)
test
str(test)
test$sigmas
map(test$sigmas, min)
map(test$sigmas, which.min)
splits <- split(dfs[[4]]$train, dfs[[4]]$train$target)
at_depth(split, 1, map, zero_variance)
at_depth(split, 1, zero_variance)
at_depth(splits, 1, map, zero_variance, .thresh = 1e-6)
at_depth(splits, 1, map_lgl, zero_variance, .thresh = 1e-6)
test  <- at_depth(splits, 1, map_lgl, zero_variance, .thresh = 1e-6)
map(test, which)
splits <- split(dfs[[4]]$train[-58], dfs[[4]]$train$target)
at_depth(splits, 1, map_lgl, zero_variance, .thresh = 1e-6)
test  <- at_depth(splits, 1, map_lgl, zero_variance, .thresh = 1e-6)
test
map(test, which)
map(test, any)
any(test)
any(flatten(test))
any(unlist(test))
!reduce(test, `+`)
reduce(test, `+`)
which(reduce(test, `+`))
which(reduce(test, `&`))
which(!!reduce(test, `+`))
# Implementing Spam experiment
# Prelims -----------------------------------------------------------------
library(tidyr)
library(dplyr)
library(purrr)
library(readr)
library(knitr)
library(ggplot2)
library(caret)
library(adventureR)
options(digits = 4)
# Links -------------------------------------------------------------------
spam_http <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spam_names <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"
# Get Data ----------------------------------------------------------------
col_names <- read_lines(spam_names, skip = 33) %>% gsub(":.*", "", .) %>% c("target")
col_types <- c(replicate(55, col_double()), replicate(3, col_integer()))
spam_data <- read_csv(spam_http, col_names = col_names, col_types = col_types)
# Fix target --------------------------------------------------------------
spam_data <- mutate(spam_data, target = as.factor(target))
# Basic EDA ---------------------------------------------------------------
summary_funs <- list(n = length,
nmiss = compose(sum, is.na),
mean = partial(mean, na.rm = TRUE),
sd = partial(sd, na.rm = TRUE),
min = partial(min, na.rm = TRUE),
p25 = partial(quantile, probs = .25, names = FALSE, na.rm = TRUE),
median = partial(median, na.rm = TRUE),
p75 = partial(quantile, probs = .75, names = FALSE, na.rm = TRUE),
max = partial(max, na.rm = TRUE))
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
invoke(rbind.data.frame, .) %>%
kable
spam_data %>% count(target) %>% kable
# Implementing the Experiment ---------------------------------------------
# Store models to be tested
TC <- trainControl(method = 'none')
NB_pars <- data.frame(fL = 0, usekernel = FALSE)
proc <- c("nzv")
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC, preProcess = proc),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
# Create list of random training and test dataframes
dfs <- rerun(10, split_data(spam_data))
# Fit and score models
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
# Lets make a nice plot
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
warnings()
test <- train(target ~ . , data = dfs[[1]]$train, method = 'lda', trControl = TC)
predict(test)
predict(test, newdata = dfs[[1]]$test)
score_model(test, newdata = dfs[[1]]$test, actual = dfs[[1]]$test$target)
test2  <- LDA(target ~ . , data = dfs[[1]]$train)
score_model(test2, newdata = dfs[[1]]$test, actual = dfs[[1]]$test$target)
results
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration")
score_model(test2, newdata = dfs[[1]]$test, actual = dfs[[1]]$test$target)
score_model(test, newdata = dfs[[1]]$test, actual = dfs[[1]]$test$target)
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
# Implementing Spam experiment
# Prelims -----------------------------------------------------------------
library(dplyr)
library(purrr)
library(readr)
library(knitr)
library(ggplot2)
library(caret)
library(adventureR)
options(digits = 4)
# Links -------------------------------------------------------------------
spam_http <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spam_names <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"
# Get Data ----------------------------------------------------------------
col_names <- read_lines(spam_names, skip = 33) %>% gsub(":.*", "", .) %>% c("target")
col_types <- c(replicate(55, col_double()), replicate(3, col_integer()))
spam_data <- read_csv(spam_http, col_names = col_names, col_types = col_types)
# Fix target --------------------------------------------------------------
spam_data <- mutate(spam_data, target = as.factor(target))
# Basic EDA ---------------------------------------------------------------
summary_funs <- list(n = length,
nmiss = compose(sum, is.na),
mean = partial(mean, na.rm = TRUE),
sd = partial(sd, na.rm = TRUE),
min = partial(min, na.rm = TRUE),
p25 = partial(quantile, probs = .25, names = FALSE, na.rm = TRUE),
median = partial(median, na.rm = TRUE),
p75 = partial(quantile, probs = .75, names = FALSE, na.rm = TRUE),
max = partial(max, na.rm = TRUE))
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
invoke(rbind.data.frame, .) %>%
kable
spam_data %>% count(target) %>% kable
# Implementing the Experiment ---------------------------------------------
# Store models to be tested
TC <- trainControl(method = 'none')
NB_pars <- data.frame(fL = 0, usekernel = FALSE)
proc <- c("nzv")
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
# Create list of random training and test dataframes
dfs <- rerun(10, split_data(spam_data))
# Fit and score models
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
# Lets make a nice plot
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
# Implementing Spam experiment
# Prelims -----------------------------------------------------------------
library(dplyr)
library(purrr)
library(readr)
library(knitr)
library(ggplot2)
library(caret)
library(adventureR)
options(digits = 4)
# Links -------------------------------------------------------------------
spam_http <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spam_names <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"
# Get Data ----------------------------------------------------------------
col_names <- read_lines(spam_names, skip = 33) %>% gsub(":.*", "", .) %>% c("target")
col_types <- c(replicate(55, col_double()), replicate(3, col_integer()))
spam_data <- read_csv(spam_http, col_names = col_names, col_types = col_types)
# Fix target --------------------------------------------------------------
spam_data <- mutate(spam_data, target = as.factor(target))
# Basic EDA ---------------------------------------------------------------
summary_funs <- list(n = length,
nmiss = compose(sum, is.na),
mean = partial(mean, na.rm = TRUE),
sd = partial(sd, na.rm = TRUE),
min = partial(min, na.rm = TRUE),
p25 = partial(quantile, probs = .25, names = FALSE, na.rm = TRUE),
median = partial(median, na.rm = TRUE),
p75 = partial(quantile, probs = .75, names = FALSE, na.rm = TRUE),
max = partial(max, na.rm = TRUE))
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
invoke(rbind.data.frame, .) %>%
kable
spam_data %>% count(target) %>% kable
# Implementing the Experiment ---------------------------------------------
# Store models to be tested
TC <- trainControl(method = 'none')
NB_pars <- data.frame(fL = 0, usekernel = FALSE)
proc <- c("nzv")
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
# Create list of random training and test dataframes
dfs <- rerun(100, split_data(spam_data))
# Fit and score models
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
# Lets make a nice plot
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
getwd()
saveRDS(results, "post-elements/2015-12-02-spam-results.rds")
file.exists("post-elements/2015-12-02-spam-results.rds")
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
# Implementing Spam experiment
# Prelims -----------------------------------------------------------------
library(dplyr)
library(purrr)
library(readr)
library(knitr)
library(ggplot2)
library(caret)
library(adventureR)
options(digits = 4)
# Links -------------------------------------------------------------------
spam_http <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
spam_names <- "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names"
# Get Data ----------------------------------------------------------------
col_names <- read_lines(spam_names, skip = 33) %>% gsub(":.*", "", .) %>% c("target")
col_types <- c(replicate(55, col_double()), replicate(3, col_integer()))
spam_data <- read_csv(spam_http, col_names = col_names, col_types = col_types)
# Fix target --------------------------------------------------------------
spam_data <- mutate(spam_data, target = as.factor(target))
# Basic EDA ---------------------------------------------------------------
summary_funs <- list(n = length,
nmiss = compose(sum, is.na),
mean = partial(mean, na.rm = TRUE),
sd = partial(sd, na.rm = TRUE),
min = partial(min, na.rm = TRUE),
p25 = partial(quantile, probs = .25, names = FALSE, na.rm = TRUE),
median = partial(median, na.rm = TRUE),
p75 = partial(quantile, probs = .75, names = FALSE, na.rm = TRUE),
max = partial(max, na.rm = TRUE))
spam_data[-58] %>% map(~ invoke_map(summary_funs, x = .x)) %>%
invoke(rbind.data.frame, .) %>%
kable
spam_data %>% count(target) %>% kable
# Implementing the Experiment ---------------------------------------------
# Store models to be tested
TC <- trainControl(method = 'none')
NB_pars <- data.frame(fL = 0, usekernel = FALSE)
proc <- c("nzv")
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
# Create list of random training and test dataframes
train(target ~ . , spam_data, method = 'qda')
train(target ~ . , spam_data, method = 'lda')
train(target ~ . , spam_data, method = 'qda') %>% confusionMatrix
train(target ~ . , spam_data, method = 'qda') %>% predict %>% confusionMatrix
train(target ~ . , spam_data, method = 'qda') %>% predict
train(target ~ . , spam_data, method = 'qda') %>% predict %>% confusionMatrix(spam_data$target)
train(target ~ . , spam_data, method = 'lda') %>% predict %>% confusionMatrix(spam_data$target)
f1_score(0.962 ,0.754)
f1_score(0.873 ,  0.955  )
dfs <- rerun(100, split_data(spam_data))
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
caret_qda = partial(train, target ~ ., method = 'qda',
trControl = TC),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
models <- list(my_lda = partial(LDA, target ~ .),
caret_lda = partial(train, target ~ ., method = 'lda',
trControl = TC),
caret_qda = partial(train, target ~ ., method = 'qda',
trControl = TC, preProcess = proc),
my_nb = partial(naive_bayes, target ~ .),
caret_nb = partial(train, target ~ ., method = 'nb',
trControl = TC, tuneGrid = NB_pars,
preProcess = proc))
results <- dfs %>%
map( ~ invoke_map(models, data = .x$train)) %>%
map2(dfs, ~ map(.x, score_model, newdata = .y$test,
actual = .y$test$target))
saveRDS(results, "post-elements/2015-12-02-spam-results.rds")
results %>% at_depth(2, data.frame) %>%
at_depth(1, bind_rows, .id = "model") %>%
bind_rows(.id = "iteration") %>%
ggplot(aes(model, f1)) +
geom_boxplot() +
ggtitle("Comparison of F1 in classification models")
mus <- list(mu1 = c(0,0), mu2 = c(2,2))
Sigma <- matrix(2, -1, -1, 2, nrow = 2)
Sigma <- matrix(c(2, -1, -1, 2), nrow = 2)
Sigma
map(mus, ~ MASS::mvrnorm(n = 25, mu = .x, Sigma = Sigma)) %>%
map(as.data.frame)
mus <- list(group1 = c(0,0), group2 = c(2,2))
map(mus, ~ MASS::mvrnorm(n = 25, mu = .x, Sigma = Sigma)) %>%
map(as.data.frame)
example_data <- map(mus, ~ MASS::mvrnorm(n = 25, mu = .x, Sigma = Sigma)) %>%
map(as.data.frame) %>%
bind_row(.id = "group")
example_data <- map(mus, ~ MASS::mvrnorm(n = 25, mu = .x, Sigma = Sigma)) %>%
map(as.data.frame) %>%
bind_rows(.id = "group")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
scale_color_brewer(palette = "Set1")
example_data <- map(mus, ~ MASS::mvrnorm(n = 50, mu = .x, Sigma = Sigma)) %>%
map(as.data.frame) %>%
bind_rows(.id = "group")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
scale_color_brewer(palette = "Set1")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
example_lda <- LDA(group ~ . , example_data)
str(example_lda)
slope <- -1 * reduce(example_lda$a, `/`)
solpe
slope
intercept <- example_lda$b %>% as.numeric
intercept
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(slope = slope, intercept = intercept, lty = 2) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
slope <- -1 * reduce(example_lda$a, `/`)
intercept <- -1 * example_lda$b %>% as.numeric
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(slope = slope, intercept = intercept, lty = 2) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
intercept <- -1 * example_lda$b %>% as.numeric %>% `/`(example_lda$a[2])
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(slope = slope, intercept = intercept, lty = 2) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(aes(lty = 2), slope = slope, intercept = intercept) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(aes(lty = "2"), slope = slope, intercept = intercept) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(aes(color = "discriminat"), slope = slope, intercept = intercept, lty = 2) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
ggplot(example_data, aes(V1, V2)) +
geom_point(aes(color = group)) +
geom_abline(aes(lty = "discriminat"), slope = slope, intercept = intercept, lty = 2) +
scale_color_brewer(palette = "Set1") +
ggtitle("LDA Example")
getwd()
